{"paragraphs":[{"text":"%md ## Spark Streaming\n\n&copy;2016, 2017 by Adam Breindel. All Rights Reserved.","user":"anonymous","dateUpdated":"2017-02-19T11:24:20-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487528716707_1493906859","id":"20170219-102516_867721242","dateCreated":"2017-02-19T10:25:16-0800","dateStarted":"2017-02-19T11:24:20-0800","dateFinished":"2017-02-19T11:24:20-0800","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:41779","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Spark Streaming</h2>\n<p>&copy;2016, 2017 by Adam Breindel. All Rights Reserved.</p>\n</div>"}]}},{"text":"%md #### Structured Streaming vs. DStream Streaming\n\n* DStream (\"classic\")\n    * Programming model and workarounds\n    * Operational models (receivers, receiverless)\n    * Fault-tolerance considerations\n* Structured Streaming (\"streaming DataFrames\")\n    * Goals\n    * Advantages and Limitations\n\n#### Common Factors\n\n  * High throughput\n  * Microbatch processing\n  * *Not* low latency (due to job scheduling)\n  * Ideal for bulk computation, not per-event isolated computation","user":"anonymous","dateUpdated":"2017-02-19T11:24:22-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487528818769_-2045889767","id":"20170219-102658_1868081552","dateCreated":"2017-02-19T10:26:58-0800","dateStarted":"2017-02-19T11:24:22-0800","dateFinished":"2017-02-19T11:24:22-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:41780","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Structured Streaming vs. DStream Streaming</h4>\n<ul>\n  <li>DStream (&ldquo;classic&rdquo;)\n    <ul>\n      <li>Programming model and workarounds</li>\n      <li>Operational models (receivers, receiverless)</li>\n      <li>Fault-tolerance considerations</li>\n    </ul>\n  </li>\n  <li>Structured Streaming (&ldquo;streaming DataFrames&rdquo;)\n    <ul>\n      <li>Goals</li>\n      <li>Advantages and Limitations</li>\n    </ul>\n  </li>\n</ul>\n<h4>Common Factors</h4>\n<ul>\n  <li>High throughput</li>\n  <li>Microbatch processing</li>\n  <li><em>Not</em> low latency (due to job scheduling)</li>\n  <li>Ideal for bulk computation, not per-event isolated computation</li>\n</ul>\n</div>"}]}},{"text":"%md __DStreams__\n\n<img src=\"http://i.imgur.com/Z2lp8Il.png\" width=800></img>","user":"anonymous","dateUpdated":"2017-02-19T11:24:25-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487528858026_1923842115","id":"20170219-102738_2063400792","dateCreated":"2017-02-19T10:27:38-0800","dateStarted":"2017-02-19T11:24:25-0800","dateFinished":"2017-02-19T11:24:25-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:41781","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><strong>DStreams</strong></p>\n<img src=\"http://i.imgur.com/Z2lp8Il.png\" width=800></img>\n</div>"}]}},{"text":"%md __General Receiver-Based Model__\n\n(run this next cell just to set up a data source; don't worry about the code)","user":"anonymous","dateUpdated":"2017-02-19T11:24:26-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487529050844_-1476509012","id":"20170219-103050_1264893885","dateCreated":"2017-02-19T10:30:50-0800","dateStarted":"2017-02-19T11:24:26-0800","dateFinished":"2017-02-19T11:24:27-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:41782","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><strong>General Receiver-Based Model</strong></p>\n<p>(run this next cell just to set up a data source; don&rsquo;t worry about the code)</p>\n</div>"}]}},{"text":"import org.apache.spark.streaming._ \n\nclass DemoSource() extends org.apache.spark.streaming.receiver.Receiver[(Int,Int)]( org.apache.spark.storage.StorageLevel.MEMORY_ONLY) {  \n  def onStart() {    \n    new Thread(\"Demo Receiver\") {\n      override def run() { genData() }\n    }.start()\n  }\n\n  def onStop() { }\n  \n  private def genData() {    \n    val r = scala.util.Random\n    val customers = 100 to 120\n    while(!isStopped) {\n      store( (customers(r.nextInt(customers.size)), 40 + r.nextInt(50)) )\n      Thread.sleep(20 + r.nextInt(40))\n    }\n  }\n}","user":"anonymous","dateUpdated":"2017-02-19T11:24:29-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487529075627_2059379352","id":"20170219-103115_1642824979","dateCreated":"2017-02-19T10:31:15-0800","dateStarted":"2017-02-19T11:24:29-0800","dateFinished":"2017-02-19T11:24:30-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:41783","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.spark.streaming._\n\ndefined class DemoSource\n"}]}},{"text":"%md What does that source produce? Key-value data, where the key represents a user ID and the value a piece of sensor data (freeway speed).\n\nThe records look like this:\n```\n(101, 55)\n(110, 60)\n(103, 63)\n(101, 59)\n```\netc.","user":"anonymous","dateUpdated":"2017-02-19T11:24:32-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487529085883_-298361907","id":"20170219-103125_1719703299","dateCreated":"2017-02-19T10:31:25-0800","dateStarted":"2017-02-19T11:24:32-0800","dateFinished":"2017-02-19T11:24:32-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:41784","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>What does that source produce? Key-value data, where the key represents a user ID and the value a piece of sensor data (freeway speed).</p>\n<p>The records look like this:</p>\n<pre><code>(101, 55)\n(110, 60)\n(103, 63)\n(101, 59)\n</code></pre>\n<p>etc.</p>\n</div>"}]}},{"text":"%md Our first streaming code:","user":"anonymous","dateUpdated":"2017-02-19T11:24:35-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487529357610_-300687370","id":"20170219-103557_183939056","dateCreated":"2017-02-19T10:35:57-0800","dateStarted":"2017-02-19T11:24:35-0800","dateFinished":"2017-02-19T11:24:35-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:41785","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Our first streaming code:</p>\n</div>"}]}},{"text":"val ssc = new StreamingContext(sc, Seconds(1))\nval stream = ssc.receiverStream(new DemoSource())\n\nssc.start\nThread.sleep(4000)\nssc.stop(false)","user":"anonymous","dateUpdated":"2017-02-19T10:36:16-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":true},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487529364394_-93292388","id":"20170219-103604_684175870","dateCreated":"2017-02-19T10:36:04-0800","dateStarted":"2017-02-19T10:36:11-0800","dateFinished":"2017-02-19T10:36:13-0800","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41786"},{"text":"%md Doh!","user":"anonymous","dateUpdated":"2017-02-19T11:24:37-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487529371473_-185616800","id":"20170219-103611_558192802","dateCreated":"2017-02-19T10:36:11-0800","dateStarted":"2017-02-19T11:24:37-0800","dateFinished":"2017-02-19T11:24:37-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:41787","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Doh!</p>\n</div>"}]}},{"text":"val ssc = new StreamingContext(sc, Seconds(1))\nval stream = ssc.receiverStream(new DemoSource())\n\nstream.foreachRDD { //foreachRDD is a DStream action\n  rdd => println(\"Received \" + rdd.count /* RDD action */ + \" data records at \" + java.util.Calendar.getInstance.getTimeInMillis)\n}\n\nssc.start\nThread.sleep(4000)\nssc.stop(false)","user":"anonymous","dateUpdated":"2017-02-19T10:36:47-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":true},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487529395817_2013854744","id":"20170219-103635_1921904302","dateCreated":"2017-02-19T10:36:35-0800","dateStarted":"2017-02-19T10:36:38-0800","dateFinished":"2017-02-19T10:36:45-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41788"},{"text":"%md Ok, that was better ... can we add just a little more logic to the stream?","user":"anonymous","dateUpdated":"2017-02-19T11:24:40-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487529398889_-311198183","id":"20170219-103638_1788059295","dateCreated":"2017-02-19T10:36:38-0800","dateStarted":"2017-02-19T11:24:40-0800","dateFinished":"2017-02-19T11:24:40-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:41789","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Ok, that was better &hellip; can we add just a little more logic to the stream?</p>\n</div>"}]}},{"text":"stream.foreachRDD { //foreachRDD is a DStream action\n  rdd => println(\"First record was \" + rdd.first)\n}","user":"anonymous","dateUpdated":"2017-02-19T10:37:15-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":true},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487529416968_-1199183321","id":"20170219-103656_1251482232","dateCreated":"2017-02-19T10:36:56-0800","dateStarted":"2017-02-19T10:37:10-0800","dateFinished":"2017-02-19T10:37:11-0800","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41790"},{"text":"%md Nope ... but what went wrong?\n\nIs it\n1. a stream can only feed a single action or transformation? or \n2. stream was already stopped?\n\nPer the error message, the problem is #2.\n\nWe *are* allowed to attach any number of transformations and/or actions to streams, although there are performance implications.\n\nWhy? each action will require one or more jobs, and the jobs overhead is a big factor in streaming performance. That's why Spark does great with high throughput, provided the jobs are not too frequent (i.e., no low-latency).","user":"anonymous","dateUpdated":"2017-02-19T11:24:42-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487529430608_-1193766188","id":"20170219-103710_323841711","dateCreated":"2017-02-19T10:37:10-0800","dateStarted":"2017-02-19T11:24:42-0800","dateFinished":"2017-02-19T11:24:42-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:41791","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Nope &hellip; but what went wrong?</p>\n<p>Is it<br/>1. a stream can only feed a single action or transformation? or<br/>2. stream was already stopped?</p>\n<p>Per the error message, the problem is #2.</p>\n<p>We <em>are</em> allowed to attach any number of transformations and/or actions to streams, although there are performance implications.</p>\n<p>Why? each action will require one or more jobs, and the jobs overhead is a big factor in streaming performance. That&rsquo;s why Spark does great with high throughput, provided the jobs are not too frequent (i.e., no low-latency).</p>\n</div>"}]}},{"text":"val ssc = new StreamingContext(sc, Seconds(1))\nval stream = ssc.receiverStream(new DemoSource())\n\nstream.foreachRDD { //foreachRDD is a DStream action\n  rdd => println(\"Received \" + rdd.count /* RDD action */ + \" data records at \" + java.util.Calendar.getInstance.getTimeInMillis)\n}\n\nstream.foreachRDD { //foreachRDD is a DStream action\n  rdd => println(\"First record was \" + rdd.first)\n}\n\nssc.start\nThread.sleep(4000)\nssc.stop(false)","user":"anonymous","dateUpdated":"2017-02-19T11:24:55-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":true},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487529484173_1997233921","id":"20170219-103804_766593924","dateCreated":"2017-02-19T10:38:04-0800","dateStarted":"2017-02-19T11:24:45-0800","dateFinished":"2017-02-19T11:24:51-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41792"},{"text":"%md It does work ... how does the performance compare? Run the following cell with and without the comment block ...<br/> It's set to run for 30 seconds so you have time to look at the Spark Streaming GUI","user":"anonymous","dateUpdated":"2017-02-19T11:24:59-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487529509933_-1162077216","id":"20170219-103829_1746440337","dateCreated":"2017-02-19T10:38:29-0800","dateStarted":"2017-02-19T11:24:59-0800","dateFinished":"2017-02-19T11:24:59-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:41793","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>It does work &hellip; how does the performance compare? Run the following cell with and without the comment block &hellip;<br/> It&rsquo;s set to run for 30 seconds so you have time to look at the Spark Streaming GUI</p>\n</div>"}]}},{"text":"val ssc = new StreamingContext(sc, Seconds(1))\nval stream = ssc.receiverStream(new DemoSource())\n\nstream.foreachRDD { //foreachRDD is a DStream action\n  rdd => println(\"Received \" + rdd.count /* RDD action */ + \" data records at \" + java.util.Calendar.getInstance.getTimeInMillis)\n}\n\n/*\nstream.foreachRDD { //foreachRDD is a DStream action\n  rdd => println(\"First record was \" + rdd.first)\n}\n*/\n\nssc.start\nThread.sleep(30000)\nssc.stop(false)","user":"anonymous","dateUpdated":"2017-02-19T11:25:05-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487529522293_1541568630","id":"20170219-103842_977029022","dateCreated":"2017-02-19T10:38:42-0800","dateStarted":"2017-02-19T11:25:02-0800","dateFinished":"2017-02-19T11:25:34-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41794"},{"text":"%md __DStream Transformations__\n\n<img src=\"http://i.imgur.com/GnTIQv9.png\" width=800></img>\n\nTake a close look at the following cell. How is it different from the earlier counting code?\n\nIt uses the .count() stream transformation!","user":"anonymous","dateUpdated":"2017-02-19T11:25:28-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487529536316_-937737959","id":"20170219-103856_1195992770","dateCreated":"2017-02-19T10:38:56-0800","dateStarted":"2017-02-19T11:25:28-0800","dateFinished":"2017-02-19T11:25:28-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:41795","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><strong>DStream Transformations</strong></p>\n<img src=\"http://i.imgur.com/GnTIQv9.png\" width=800></img>\n<p>Take a close look at the following cell. How is it different from the earlier counting code?</p>\n<p>It uses the .count() stream transformation!</p>\n</div>"}]}},{"text":"val ssc = new StreamingContext(sc, Seconds(1))\nval stream = ssc.receiverStream(new DemoSource())\n\nstream.count.foreachRDD { // count *DStream transformation* + foreachRDD *DStream action*\n  rdd => println(\"Received \" + rdd.first /* rdd *action* */ + \" data records at \" + java.util.Calendar.getInstance.getTimeInMillis)\n}\n\nssc.start\nThread.sleep(4000)\nssc.stop(false)","user":"anonymous","dateUpdated":"2017-02-19T11:25:31-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":true},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487529571097_141143930","id":"20170219-103931_146087602","dateCreated":"2017-02-19T10:39:31-0800","dateStarted":"2017-02-19T10:44:55-0800","dateFinished":"2017-02-19T10:45:01-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41796"},{"text":"%md Before going any further with the Streaming API, let's take a quick look at a receiverless \"advanced\" source example, just so we can look for the similarities and differences.\n\n*Note: this code will not work unless you install a library (.jar). If you want an Uber-jar (i.e., JAR with dependencies), look for spark-streaming-kafka-XXXX\n\nWhere XXXX matches your versions of Spark, Scala, and Kafka. If you're running this with Zeppelin 0.7, that will be Spark 2.1, Scala 2.11, Kafka 0.8\n\n__Even if you don't want to wire up the libs to run this example, let's take a quick look at the code. It's not any more complex than what we have been doing, but with KafkaDirect, we get a ton of features that make our application more robust *and* easier to operate at the same time__\n\n1. Automatic parallelization across Kafka topic partitions\n2. No dedicated receiver cores \n3. Easy recovery in the case of brief failure, because Kafka keeps the records and we can just ask for them again (by offset range)\n    * This means we need to track offsets, but don't need to journal the actual data","user":"anonymous","dateUpdated":"2017-02-19T11:25:43-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487529895737_-609077554","id":"20170219-104455_1054520696","dateCreated":"2017-02-19T10:44:55-0800","dateStarted":"2017-02-19T11:25:43-0800","dateFinished":"2017-02-19T11:25:43-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:41797","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Before going any further with the Streaming API, let&rsquo;s take a quick look at a receiverless &ldquo;advanced&rdquo; source example, just so we can look for the similarities and differences.</p>\n<p>*Note: this code will not work unless you install a library (.jar). If you want an Uber-jar (i.e., JAR with dependencies), look for spark-streaming-kafka-XXXX</p>\n<p>Where XXXX matches your versions of Spark, Scala, and Kafka. If you&rsquo;re running this with Zeppelin 0.7, that will be Spark 2.1, Scala 2.11, Kafka 0.8</p>\n<p><strong>Even if you don&rsquo;t want to wire up the libs to run this example, let&rsquo;s take a quick look at the code. It&rsquo;s not any more complex than what we have been doing, but with KafkaDirect, we get a ton of features that make our application more robust <em>and</em> easier to operate at the same time</strong></p>\n<ol>\n  <li>Automatic parallelization across Kafka topic partitions</li>\n  <li>No dedicated receiver cores</li>\n  <li>Easy recovery in the case of brief failure, because Kafka keeps the records and we can just ask for them again (by offset range)\n    <ul>\n      <li>This means we need to track offsets, but don&rsquo;t need to journal the actual data</li>\n    </ul>\n  </li>\n</ol>\n</div>"}]}},{"text":"import _root_.kafka.serializer.StringDecoder\nimport org.apache.spark.streaming._\nimport org.apache.spark.streaming.kafka._\n\nval ssc = new StreamingContext(sc, Seconds(1))\nval kafkaParams = Map(\"metadata.broker.list\" -> \"ec2-54-201-178-163.us-west-2.compute.amazonaws.com:9092\")\nval k = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, Set(\"purchase\"))\n\nk.count.foreachRDD {\n  rdd => println(\"Received \" + rdd.first + \" data records at \" + java.util.Calendar.getInstance.getTimeInMillis)\n}\n\nk.foreachRDD {\n  rdd => println(\"Data is \" + rdd.collect.mkString(\", \") + \" at \" + java.util.Calendar.getInstance.getTimeInMillis)\n}\n\nssc.start\nThread sleep 4000\nssc.stop(false)","user":"anonymous","dateUpdated":"2017-02-19T10:50:20-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":true},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487530199477_-943963984","id":"20170219-104959_1782267802","dateCreated":"2017-02-19T10:49:59-0800","dateStarted":"2017-02-19T10:50:18-0800","dateFinished":"2017-02-19T10:50:18-0800","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41798"},{"text":"%md The DStream API includes additional operations like reduceByKey, union, and join. \nIf we need a DStream transformation which doesn't exist in the API, we call dstream.transform(...) \nand supply an arbitrary function that takes a RDD (for each batch) and returns a transformed RDD.\n","user":"anonymous","dateUpdated":"2017-02-19T11:25:49-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487530218428_1930525697","id":"20170219-105018_239122416","dateCreated":"2017-02-19T10:50:18-0800","dateStarted":"2017-02-19T11:25:49-0800","dateFinished":"2017-02-19T11:25:50-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:41799","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>The DStream API includes additional operations like reduceByKey, union, and join.<br/>If we need a DStream transformation which doesn&rsquo;t exist in the API, we call dstream.transform(&hellip;)<br/>and supply an arbitrary function that takes a RDD (for each batch) and returns a transformed RDD.</p>\n</div>"}]}},{"text":"%md #### Stateful Streams\n\nSo far, all of the streams we've used involve computation only on the current batch (in one or more streams).\n\nSome applications require more complex computation, such as calculating moving averages over several batched, or \"sessionization\" -- collected all of the data for a user over an indefinite period of time until some business rule indicates the end of the \"session\" (e.g., user logout or idle time).\n\nTo accommodate these scenarios, Spark has support for two different kinds of stateful streams.","user":"anonymous","dateUpdated":"2017-02-19T11:25:53-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487530336194_-1084581222","id":"20170219-105216_1655620219","dateCreated":"2017-02-19T10:52:16-0800","dateStarted":"2017-02-19T11:25:53-0800","dateFinished":"2017-02-19T11:25:53-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:41800","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Stateful Streams</h4>\n<p>So far, all of the streams we&rsquo;ve used involve computation only on the current batch (in one or more streams).</p>\n<p>Some applications require more complex computation, such as calculating moving averages over several batched, or &ldquo;sessionization&rdquo; &ndash; collected all of the data for a user over an indefinite period of time until some business rule indicates the end of the &ldquo;session&rdquo; (e.g., user logout or idle time).</p>\n<p>To accommodate these scenarios, Spark has support for two different kinds of stateful streams.</p>\n</div>"}]}},{"text":"%md __Windowed Streams__\n\n* .window(...)\n* \"andWindow\" methods: reduceByKeyAndWindow etc\n\n<img src=\"http://i.imgur.com/COBtkrV.png\" width=800></img>","user":"anonymous","dateUpdated":"2017-02-19T11:25:58-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487530357001_2118884092","id":"20170219-105237_1084108607","dateCreated":"2017-02-19T10:52:37-0800","dateStarted":"2017-02-19T11:25:58-0800","dateFinished":"2017-02-19T11:25:58-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:41801","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><strong>Windowed Streams</strong></p>\n<ul>\n  <li>.window(&hellip;)</li>\n  <li>&ldquo;andWindow&rdquo; methods: reduceByKeyAndWindow etc</li>\n</ul>\n<img src=\"http://i.imgur.com/COBtkrV.png\" width=800></img>\n</div>"}]}},{"text":"val ssc = new StreamingContext(sc, Seconds(1))\nval stream = ssc.receiverStream(new DemoSource()).map(_._2)\n\nstream.window(Seconds(3)).count.foreachRDD {\n  rdd => println(\"Received \" + rdd.first + \" data records at \" + java.util.Calendar.getInstance.getTimeInMillis)\n}\n\nstream.window(Seconds(4), Seconds(4)).foreachRDD {\n  rdd => println(\"Average speed is \" + rdd.mean + \" at \" + java.util.Calendar.getInstance.getTimeInMillis)\n}\n\nssc.start\nThread.sleep(12000)\nssc.stop(false)","user":"anonymous","dateUpdated":"2017-02-19T10:53:30-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":true},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487530369769_1580417878","id":"20170219-105249_1399117207","dateCreated":"2017-02-19T10:52:49-0800","dateStarted":"2017-02-19T10:53:27-0800","dateFinished":"2017-02-19T10:53:40-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41802"},{"text":"%md __Long-Term Stateful Streams__\n\n* Collect data on per key (user, device, etc.) basis\n* Any type of data\n* Any type of collection structure\n* (Almost) any business rules\n\nupdateStateByKey:","user":"anonymous","dateUpdated":"2017-02-19T11:26:01-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487530378560_-1703413985","id":"20170219-105258_820870918","dateCreated":"2017-02-19T10:52:58-0800","dateStarted":"2017-02-19T11:26:01-0800","dateFinished":"2017-02-19T11:26:01-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:41803","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><strong>Long-Term Stateful Streams</strong></p>\n<ul>\n  <li>Collect data on per key (user, device, etc.) basis</li>\n  <li>Any type of data</li>\n  <li>Any type of collection structure</li>\n  <li>(Almost) any business rules</li>\n</ul>\n<p>updateStateByKey:</p>\n</div>"}]}},{"text":"val ssc = new StreamingContext(sc, Seconds(1))\nval stream = ssc.receiverStream(new DemoSource())\n\nssc.checkpoint(\"/tmp\")\n\nstream.updateStateByKey( (newData:Seq[Int], oldData:Option[Iterable[Any]]) => {\n  oldData match {\n    case None => Some(newData.toVector)\n    case Some(existingData) => Some(existingData ++ newData)\n  }\n})\n  .foreachRDD {\n  rdd => println(\"Time: \" + java.util.Calendar.getInstance.getTimeInMillis + \"\\nData: \" + rdd.collect.mkString(\", \") + \"\\n------------\\n\")\n}\n\nssc.start\nThread.sleep(4000)\nssc.stop(false)","user":"anonymous","dateUpdated":"2017-02-19T10:53:54-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":true},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487530424615_-1808789211","id":"20170219-105344_809976093","dateCreated":"2017-02-19T10:53:44-0800","dateStarted":"2017-02-19T10:53:52-0800","dateFinished":"2017-02-19T10:53:58-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41804"},{"text":"%md Similar, slightly more complex and possibly more performant, call .mapWithState is also available\n\nhttps://docs.cloud.databricks.com/docs/latest/databricks_guide/07%20Spark%20Streaming/12%20Global%20Aggregations%20-%20mapWithState.html","user":"anonymous","dateUpdated":"2017-02-19T11:26:06-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487530432047_1038968906","id":"20170219-105352_615285554","dateCreated":"2017-02-19T10:53:52-0800","dateStarted":"2017-02-19T11:26:06-0800","dateFinished":"2017-02-19T11:26:06-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:41805","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Similar, slightly more complex and possibly more performant, call .mapWithState is also available</p>\n<p><a href=\"https://docs.cloud.databricks.com/docs/latest/databricks_guide/07%20Spark%20Streaming/12%20Global%20Aggregations%20-%20mapWithState.html\">https://docs.cloud.databricks.com/docs/latest/databricks_guide/07%20Spark%20Streaming/12%20Global%20Aggregations%20-%20mapWithState.html</a></p>\n</div>"}]}},{"text":"%md __Combining Legacy Streaming with SQL/DataFrame__\n\nConvert each batch into a DataFrame\n\nThis example is stateless in the sense that the DataFrame is replaced anew for each batch -- simple but limited","user":"anonymous","dateUpdated":"2017-02-19T11:26:10-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487530449222_498796775","id":"20170219-105409_1017960737","dateCreated":"2017-02-19T10:54:09-0800","dateStarted":"2017-02-19T11:26:10-0800","dateFinished":"2017-02-19T11:26:10-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:41806","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><strong>Combining Legacy Streaming with SQL/DataFrame</strong></p>\n<p>Convert each batch into a DataFrame</p>\n<p>This example is stateless in the sense that the DataFrame is replaced anew for each batch &ndash; simple but limited</p>\n</div>"}]}},{"text":"val ssc = new StreamingContext(sc, Seconds(1))\nval stream = ssc.receiverStream(new DemoSource())\n\nstream.foreachRDD {\n  rdd => rdd.map( speed => (speed._1, speed._2, java.util.Calendar.getInstance.getTimeInMillis) )\n         .toDF(\"customer\", \"speed\", \"processingTime\").createOrReplaceTempView(\"speeds\")\n}\n\nssc.start\nThread.sleep(4000)\nssc.stop(false)\n\nspark.table(\"speeds\").show(100)","user":"anonymous","dateUpdated":"2017-02-19T10:54:37-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":true},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487530463590_1984727675","id":"20170219-105423_2111190231","dateCreated":"2017-02-19T10:54:23-0800","dateStarted":"2017-02-19T10:54:31-0800","dateFinished":"2017-02-19T10:54:38-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41807"},{"text":"%md This approach can be useful; if we want to query over a larger (longer-term) collection of data ... say, cumulative collection of records,\nor hours of records, it usually a good idea to write the data out to another system. Depending on your requirements, this might be a filesystem (HDFS, S3, etc.) or a database that supports queries, like Cassandra.","user":"anonymous","dateUpdated":"2017-02-19T11:26:12-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487530471894_-1099419508","id":"20170219-105431_309429721","dateCreated":"2017-02-19T10:54:31-0800","dateStarted":"2017-02-19T11:26:12-0800","dateFinished":"2017-02-19T11:26:12-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:41808","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>This approach can be useful; if we want to query over a larger (longer-term) collection of data &hellip; say, cumulative collection of records,<br/>or hours of records, it usually a good idea to write the data out to another system. Depending on your requirements, this might be a filesystem (HDFS, S3, etc.) or a database that supports queries, like Cassandra.</p>\n</div>"}]}},{"text":"%md ### What about all of the operational considerations?\n\n#### Fault Tolerance\n* Reliable Receivers\n* Write-Ahead Log\n* Data Checkpointing\n\n#### High Availability\n* Driver Recovery\n* Metadata Checkpointing\n\n#### Exactly-Once End-to-End Message Processing\n* https://databricks.com/blog/2015/01/15/improved-driver-fault-tolerance-and-zero-data-loss-in-spark-streaming.html\n* http://blog.cloudera.com/blog/2015/03/exactly-once-spark-streaming-from-apache-kafka/","user":"anonymous","dateUpdated":"2017-02-19T11:26:17-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487530637214_828201485","id":"20170219-105717_551511967","dateCreated":"2017-02-19T10:57:17-0800","dateStarted":"2017-02-19T11:26:17-0800","dateFinished":"2017-02-19T11:26:17-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:41809","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>What about all of the operational considerations?</h3>\n<h4>Fault Tolerance</h4>\n<ul>\n  <li>Reliable Receivers</li>\n  <li>Write-Ahead Log</li>\n  <li>Data Checkpointing</li>\n</ul>\n<h4>High Availability</h4>\n<ul>\n  <li>Driver Recovery</li>\n  <li>Metadata Checkpointing</li>\n</ul>\n<h4>Exactly-Once End-to-End Message Processing</h4>\n<ul>\n  <li><a href=\"https://databricks.com/blog/2015/01/15/improved-driver-fault-tolerance-and-zero-data-loss-in-spark-streaming.html\">https://databricks.com/blog/2015/01/15/improved-driver-fault-tolerance-and-zero-data-loss-in-spark-streaming.html</a></li>\n  <li><a href=\"http://blog.cloudera.com/blog/2015/03/exactly-once-spark-streaming-from-apache-kafka/\">http://blog.cloudera.com/blog/2015/03/exactly-once-spark-streaming-from-apache-kafka/</a></li>\n</ul>\n</div>"}]}},{"text":"%md #### Hmm...\n\n#### What if we could combine streaming with DataFrames, optimized state tracking for aggregations, and simplified fault tolerance with end-to-end guarantees?\n","user":"anonymous","dateUpdated":"2017-02-19T11:26:21-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487530694708_351390329","id":"20170219-105814_1183750754","dateCreated":"2017-02-19T10:58:14-0800","dateStarted":"2017-02-19T11:26:21-0800","dateFinished":"2017-02-19T11:26:21-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:41810","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Hmm&hellip;</h4>\n<h4>What if we could combine streaming with DataFrames, optimized state tracking for aggregations, and simplified fault tolerance with end-to-end guarantees?</h4>\n</div>"}]}},{"text":"%md ### Structured Streaming\n\n* Fault tolerance\n* Available source/sink strategies\n* Incremental query optimization\n\n\"Easiest way to reason about streaming is to ... not reason about streaming\"\n\n* Treat sources as \"append-only\" tables\n* Read and manipulate with DataFrame API\n* Internal resilient state management for aggregations over time\n* Output to sinks","user":"anonymous","dateUpdated":"2017-02-19T11:26:24-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487530819506_-1232636217","id":"20170219-110019_800101349","dateCreated":"2017-02-19T11:00:19-0800","dateStarted":"2017-02-19T11:26:24-0800","dateFinished":"2017-02-19T11:26:24-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:41811","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Structured Streaming</h3>\n<ul>\n  <li>Fault tolerance</li>\n  <li>Available source/sink strategies</li>\n  <li>Incremental query optimization</li>\n</ul>\n<p>&ldquo;Easiest way to reason about streaming is to &hellip; not reason about streaming&rdquo;</p>\n<ul>\n  <li>Treat sources as &ldquo;append-only&rdquo; tables</li>\n  <li>Read and manipulate with DataFrame API</li>\n  <li>Internal resilient state management for aggregations over time</li>\n  <li>Output to sinks</li>\n</ul>\n</div>"}]}},{"text":"%md To keep the environment simple for this class, and save time,\nwe'll use a socket source and a memory sink to demonstrate Structured Streaming. \nIn production, it is essential to use fault-tolerant sources and sinks such as the Kafka source and the filesystem sink.\n","user":"anonymous","dateUpdated":"2017-02-19T11:26:28-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487531153021_220421340","id":"20170219-110553_1684115231","dateCreated":"2017-02-19T11:05:53-0800","dateStarted":"2017-02-19T11:26:28-0800","dateFinished":"2017-02-19T11:26:28-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:41812","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>To keep the environment simple for this class, and save time,<br/>we&rsquo;ll use a socket source and a memory sink to demonstrate Structured Streaming.<br/>In production, it is essential to use fault-tolerant sources and sinks such as the Kafka source and the filesystem sink.</p>\n</div>"}]}},{"text":"%sql SET spark.sql.shuffle.partitions = 3 ","user":"anonymous","dateUpdated":"2017-02-19T11:07:44-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sql","editOnDblClick":true},"editorMode":"ace/mode/sql","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487530861030_-1584019232","id":"20170219-110101_959914080","dateCreated":"2017-02-19T11:01:01-0800","dateStarted":"2017-02-19T11:07:33-0800","dateFinished":"2017-02-19T11:07:34-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41813"},{"text":"%sql\n","user":"anonymous","dateUpdated":"2017-02-19T11:21:33-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sql"},"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487532093913_784440802","id":"20170219-112133_1386192119","dateCreated":"2017-02-19T11:21:33-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41814"},{"text":"val lines = spark.readStream\n  .format(\"socket\")\n  .option(\"host\", \"54.213.33.240\")\n  .option(\"port\", 9002)\n  .load()\n  \nval edits = lines.select(json_tuple('value, \"channel\", \"timestamp\", \"isRobot\", \"isAnonymous\"))\n                .selectExpr(\"c0 as channel\", \"c1 as time\", \"c2 as robot\", \"c3 as anon\")\n\nval query = edits.writeStream\n  .queryName(\"demo\")\n  .outputMode(\"append\")\n  .format(\"memory\")\n  .start()","user":"anonymous","dateUpdated":"2017-02-19T11:22:03-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487531005864_177715857","id":"20170219-110325_954460005","dateCreated":"2017-02-19T11:03:25-0800","dateStarted":"2017-02-19T11:22:03-0800","dateFinished":"2017-02-19T11:22:04-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41815"},{"text":"%sql SELECT * FROM demo","user":"anonymous","dateUpdated":"2017-02-19T11:22:13-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sql"},"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487531306339_2046686451","id":"20170219-110826_1839713258","dateCreated":"2017-02-19T11:08:26-0800","dateStarted":"2017-02-19T11:22:13-0800","dateFinished":"2017-02-19T11:22:13-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41816"},{"text":"query.stop","user":"anonymous","dateUpdated":"2017-02-19T11:22:18-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487531325427_684305920","id":"20170219-110845_611901439","dateCreated":"2017-02-19T11:08:45-0800","dateStarted":"2017-02-19T11:22:18-0800","dateFinished":"2017-02-19T11:22:19-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41817"},{"text":"%md We'll try something a little more practical.\n\n* Interpret the time as SQL timestamp (it was a raw string earlier)\n* Aggregate over a time window\n* Transform the stream by grouping by channel and time, then counting edits\n* Specfiy a trigger interval","user":"anonymous","dateUpdated":"2017-02-19T11:26:46-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487531346768_-1722451692","id":"20170219-110906_528567220","dateCreated":"2017-02-19T11:09:06-0800","dateStarted":"2017-02-19T11:26:46-0800","dateFinished":"2017-02-19T11:26:46-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:41818","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>We&rsquo;ll try something a little more practical.</p>\n<ul>\n  <li>Interpret the time as SQL timestamp (it was a raw string earlier)</li>\n  <li>Aggregate over a time window</li>\n  <li>Transform the stream by grouping by channel and time, then counting edits</li>\n  <li>Specfiy a trigger interval</li>\n</ul>\n</div>"}]}},{"text":"val lines2 = spark.readStream\n  .format(\"socket\")\n  .option(\"host\", \"54.213.33.240\")\n  .option(\"port\", 9002)\n  .load()\n\nval edits2 = lines2\n                .select(json_tuple('value, \"channel\", \"timestamp\", \"page\"))\n                .selectExpr(\"c0 as channel\", \"cast(c1 as timestamp) as time\", \"c2 as page\")\n                .groupBy(window($\"time\", \"10 seconds\"), $\"channel\").count()                \n\nval query2 = edits2.writeStream\n  .queryName(\"demo2\")\n  .outputMode(\"complete\")\n  .format(\"memory\")\n  .trigger(ProcessingTime(\"10 seconds\"))\n  .start()","user":"anonymous","dateUpdated":"2017-02-19T11:23:27-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":true},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487531412880_2038727673","id":"20170219-111012_1407364053","dateCreated":"2017-02-19T11:10:12-0800","dateStarted":"2017-02-19T11:23:24-0800","dateFinished":"2017-02-19T11:23:24-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41819"},{"text":"%sql SELECT * FROM demo2","user":"anonymous","dateUpdated":"2017-02-19T11:23:33-0800","config":{"colWidth":12,"enabled":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":true,"setting":{"lineChart":{}},"commonSetting":{},"keys":[{"name":"window","index":0,"aggr":"sum"}],"groups":[{"name":"channel","index":1,"aggr":"sum"}],"values":[{"name":"channel","index":1,"aggr":"sum"}]},"helium":{}}},"editorSetting":{"language":"sql"},"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487531430006_1026468641","id":"20170219-111030_1173652410","dateCreated":"2017-02-19T11:10:30-0800","dateStarted":"2017-02-19T11:23:33-0800","dateFinished":"2017-02-19T11:23:33-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41820"},{"text":"%sql SELECT *, date_format(window.start, \"HH:mm:ss\") as time FROM demo2 ORDER BY time, channel","user":"anonymous","dateUpdated":"2017-02-19T11:23:50-0800","config":{"colWidth":12,"enabled":true,"results":{"0":{"graph":{"mode":"lineChart","height":300,"optionOpen":false,"setting":{"lineChart":{}},"commonSetting":{},"keys":[{"name":"time","index":3,"aggr":"sum"}],"groups":[{"name":"channel","index":1,"aggr":"sum"}],"values":[{"name":"count","index":2,"aggr":"sum"}]},"helium":{}}},"editorSetting":{"language":"sql"},"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487531444766_-1933389323","id":"20170219-111044_1995656321","dateCreated":"2017-02-19T11:10:44-0800","dateStarted":"2017-02-19T11:23:50-0800","dateFinished":"2017-02-19T11:23:50-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41821"},{"text":"query2.stop","user":"anonymous","dateUpdated":"2017-02-19T11:23:58-0800","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487531486182_-1514905113","id":"20170219-111126_1972350781","dateCreated":"2017-02-19T11:11:26-0800","dateStarted":"2017-02-19T11:23:58-0800","dateFinished":"2017-02-19T11:23:58-0800","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41822"},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487531645667_-2011769185","id":"20170219-111405_42341523","dateCreated":"2017-02-19T11:14:05-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41823"}],"name":"Streaming","id":"2C9F93KZA","angularObjects":{"2CAVTNYXD:shared_process":[],"2C7K2429Q:shared_process":[],"2C8SPPQZU:shared_process":[],"2C7UX75H1:shared_process":[],"2C8CZHBMK:shared_process":[],"2C8M1YA6S:shared_process":[],"2C8HBGBZH:shared_process":[],"2C8W1YSQF:shared_process":[],"2CA11FFZW:shared_process":[],"2C8GTCYUP:shared_process":[],"2C7JSZ74W:shared_process":[],"2C7NXAQD7:shared_process":[],"2C8BUSWZY:shared_process":[],"2C8BMRSH6:shared_process":[],"2CAD4S1XP:shared_process":[],"2C8DQ16J7:shared_process":[],"2C9YA18WV:shared_process":[],"2CA315FYH:shared_process":[],"2C915WF4P:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}